apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vector-aggregator
  namespace: activity-system
spec:
  interval: 1h
  timeout: 1m
  chart:
    spec:
      chart: vector
      version: 0.49.x
      sourceRef:
        kind: HelmRepository
        name: vector
        namespace: activity-system
      interval: 1h
  values:
    # Role: Aggregator (stateless, can run as Deployment)
    role: Stateless-Aggregator
    replicas: 2
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
    service:
      enabled: true
      type: ClusterIP
      ports:
        - name: metrics
          port: 9090
          protocol: TCP
    podMonitor:
      enabled: true
      port: internal-promet
    customConfig:
      data_dir: /vector-data-dir

      # API for health checks
      api:
        enabled: true
        address: 0.0.0.0:8686

      sources:
        # NATS JetStream consumer (pull-based with proper acknowledgements)
        # Vector v0.50.0+ supports JetStream with automatic message acknowledgement
        # Messages are only ACKed after successful processing through the entire pipeline
        nats_consumer:
          type: nats
          connection_name: vector-aggregator
          url: nats://nats.nats-system.svc.cluster.local:4222

          # Subject filter (required even with JetStream)
          # This filters which messages from the stream to consume
          subject: audit.k8s.>

          # JetStream configuration (enables pull-based consumption with ACKs)
          jetstream:
            # Name of the JetStream stream to consume from
            stream: AUDIT_EVENTS
            # Name of the durable consumer (must be pre-created)
            consumer: clickhouse-ingest

            batch_config:
              batch: 1000
              # Maximum 10MB per batch
              max_bytes: 10485760
          decoding:
            codec: json
        internal_metrics:
          type: internal_metrics
          namespace: vector
      transforms:
        # Calculate end-to-end latency from K8s event generation to aggregator processing.
        # The NATS source doesn't preserve .timestamp from JSON payloads, so we must
        # calculate latency here rather than relying on vector_source_lag_time_seconds.
        # This measures true pipeline latency: API Server → Sidecar → NATS → Aggregator.
        calculate_end_to_end_latency:
          type: remap
          inputs:
            - nats_consumer
          source: |
            if exists(.stageTimestamp) {
              stage_ts, parse_err = parse_timestamp(.stageTimestamp, format: "%+")

              if parse_err == null {
                # Calculate latency by converting timestamps to Unix time (seconds)
                # VRL doesn't support direct timestamp subtraction
                current_unix = to_unix_timestamp(now(), unit: "seconds")
                stage_unix = to_unix_timestamp(stage_ts, unit: "seconds")

                # Calculate latency in seconds (already in seconds, no conversion needed)
                .end_to_end_latency_seconds = current_unix - stage_unix
                .timestamp = stage_ts
              } else {
                log("Failed to parse stageTimestamp '" + string!(.stageTimestamp) + "': " + string!(parse_err), level: "warn")
                .timestamp = now()
              }
            } else {
              log("Missing stageTimestamp field in audit event", level: "warn")
              .timestamp = now()
            }

        # Convert the latency field to a histogram metric
        latency_to_metric:
          type: log_to_metric
          inputs:
            - calculate_end_to_end_latency
          metrics:
            - type: histogram
              field: end_to_end_latency_seconds
              name: end_to_end_latency_seconds
              namespace: activity_pipeline
              tags:
                stage: nats_to_aggregator
                component_id: nats_consumer

        # Removes internal cluster IP ranges (RFC 1918 private addresses)
        #
        # IPv4: 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 127.0.0.0/8
        # IPv6: ::1 (loopback), fc00::/7 (ULA), fe80::/10 (link-local)
        filter_private_ips:
          type: remap
          inputs:
            - calculate_end_to_end_latency
          source: |
            # Filter internal cluster IPs from sourceIPs array
            if exists(.sourceIPs) && is_array(.sourceIPs) {
              filtered_source_ips = []
              for_each(array!(.sourceIPs)) -> |_ip_index, source_ip| {
                ip_str = string!(source_ip)

                if !match(ip_str, r'^10\.') &&
                   !match(ip_str, r'^172\.(1[6-9]|2[0-9]|3[0-1])\.') &&
                   !match(ip_str, r'^192\.168\.') &&
                   !match(ip_str, r'^127\.') &&
                   !match(ip_str, r'^::1$') &&
                   !match(ip_str, r'^fc[0-9a-f][0-9a-f]:') &&
                   !match(ip_str, r'^fe80:') {
                  filtered_source_ips = push(filtered_source_ips, source_ip)
                }
              }
              .sourceIPs = filtered_source_ips
            }

        # Filter to ResponseComplete stage only (recommended strategy for 75% data reduction)
        # This reduces storage and query costs while keeping all necessary audit information
        filter_response_complete:
          type: filter
          inputs:
            - filter_private_ips
          condition:
            type: vrl
            source: |
              .stage == "ResponseComplete"

        # Prepare events for ClickHouse
        prepare_for_clickhouse:
          type: remap
          inputs:
            - filter_response_complete
          source: |
            # Wrap entire event as event_json for ClickHouse
            .event_json = encode_json(.)
            . = {
              "event_json": .event_json
            }

      sinks:
        # Export audit events to clickhouse
        clickhouse:
          type: clickhouse
          inputs:
            - prepare_for_clickhouse
          endpoint: http://clickhouse.activity-system.svc.cluster.local:8123
          database: audit
          table: events
          encoding:
            only_fields:
              - event_json
          batch:
            max_bytes: 10485760
            max_events: 10000
            timeout_secs: 10
          buffer:
            type: disk
            max_size: 10737418240
            when_full: block
          request:
            retry_attempts: 9999999
            retry_initial_backoff_secs: 1
            retry_max_duration_secs: 300
            timeout_secs: 60
          healthcheck:
            enabled: true
          compression: gzip

        # Export metrics over prometheus
        internal_prometheus:
          type: prometheus_exporter
          inputs:
            - internal_metrics
            - latency_to_metric
          address: 0.0.0.0:9091
          default_namespace: vector_internal
          buckets: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0, 30.0, 60.0, 120.0, 180.0, 240.0, 300.0]

    env:
      - name: CLICKHOUSE_USERNAME
        value: "default"
      - name: CLICKHOUSE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: clickhouse-credentials
            key: password
            optional: true
      - name: VECTOR_LOG
        value: "info"
      - name: VECTOR_LOG_FORMAT
        value: "json"

    # Persistence for buffer
    persistence:
      enabled: true
      storageClassName: ""  # Use default storage class
      accessModes:
        - ReadWriteOnce
      size: 10Gi

  install:
    crds: Create
    createNamespace: false

  upgrade:
    crds: CreateReplace

  uninstall:
    keepHistory: false
