# AUTO-GENERATED - DO NOT EDIT MANUALLY
# Generated from migrations/ directory (source of truth)
# To regenerate: task migrations:generate OR ./hack/generate-migrations-configmap.sh
#
# To add a new migration:
# 1. Create file in migrations/ (e.g., migrations/002_add_field.sql)
# 2. Run: task migrations:generate
# 3. Update job.yaml to include the new migration in volumes
# 4. Deploy: task dev:deploy

apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-migrations
  namespace: activity-system
  labels:
    app: clickhouse-migrations
    app.kubernetes.io/component: database
data:
  # Migration runner script
  migrate.sh: |
    #!/bin/bash
    set -euo pipefail

    # ClickHouse Migration Runner
    # This script applies versioned SQL migrations to a ClickHouse database
    # It tracks applied migrations in the audit.schema_migrations table

    # Configuration from environment variables
    CLICKHOUSE_HOST="${CLICKHOUSE_HOST:-clickhouse}"
    CLICKHOUSE_PORT="${CLICKHOUSE_PORT:-9000}"
    CLICKHOUSE_USER="${CLICKHOUSE_USER:-default}"
    CLICKHOUSE_PASSWORD="${CLICKHOUSE_PASSWORD:-}"
    CLICKHOUSE_DATABASE="${CLICKHOUSE_DATABASE:-audit}"
    MIGRATIONS_DIR="${MIGRATIONS_DIR:-/migrations}"
    CLICKHOUSE_SECURE="${CLICKHOUSE_SECURE:-false}"
    CLICKHOUSE_CLIENT_EXTRA_ARGS="${CLICKHOUSE_CLIENT_EXTRA_ARGS:-}"

    # Colors for output
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    BLUE='\033[0;34m'
    NC='\033[0m' # No Color

    log_info() {
        echo -e "${BLUE}[INFO]${NC} $1"
    }

    log_success() {
        echo -e "${GREEN}[SUCCESS]${NC} $1"
    }

    log_warning() {
        echo -e "${YELLOW}[WARNING]${NC} $1"
    }

    log_error() {
        echo -e "${RED}[ERROR]${NC} $1"
    }

    # Build clickhouse-client command with authentication
    clickhouse_cmd() {
        local query="$1"
        local cmd="clickhouse-client ${CLICKHOUSE_CLIENT_EXTRA_ARGS} --host=${CLICKHOUSE_HOST} --port=${CLICKHOUSE_PORT} --user=${CLICKHOUSE_USER}"

        if [ -n "${CLICKHOUSE_PASSWORD}" ]; then
            cmd="${cmd} --password=${CLICKHOUSE_PASSWORD}"
        fi

        echo "${query}" | ${cmd}
    }

    # Wait for ClickHouse to be ready
    wait_for_clickhouse() {
        log_info "Waiting for ClickHouse to be ready at ${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}..."

        local max_attempts=30
        local attempt=1

        while [ $attempt -le $max_attempts ]; do
            if clickhouse_cmd "SELECT 1" &>/dev/null; then
                log_success "ClickHouse is ready!"
                return 0
            fi

            log_info "Attempt $attempt/$max_attempts: ClickHouse not ready yet, waiting..."
            sleep 2
            attempt=$((attempt + 1))
        done

        log_error "ClickHouse did not become ready within the timeout period"
        return 1
    }

    # Initialize the schema_migrations table if it doesn't exist
    init_migrations_table() {
        log_info "Initializing schema_migrations table..."

        # First ensure the database exists
        clickhouse_cmd "CREATE DATABASE IF NOT EXISTS ${CLICKHOUSE_DATABASE}"

        # Create the migrations tracking table
        clickhouse_cmd "
            CREATE TABLE IF NOT EXISTS ${CLICKHOUSE_DATABASE}.schema_migrations (
                version UInt32,
                name String,
                applied_at DateTime64(3) DEFAULT now64(3),
                checksum String
            ) ENGINE = MergeTree()
            ORDER BY version
        "

        log_success "Schema migrations table is ready"
    }

    # Calculate checksum of a file
    calculate_checksum() {
        local file="$1"
        sha256sum "${file}" | awk '{print $1}'
    }

    # Check if a migration has already been applied
    is_migration_applied() {
        local version="$1"
        local result=$(clickhouse_cmd "
            SELECT count(*)
            FROM ${CLICKHOUSE_DATABASE}.schema_migrations
            WHERE version = ${version}
        ")

        [ "${result}" -gt 0 ]
    }

    # Record a migration as applied
    record_migration() {
        local version="$1"
        local name="$2"
        local checksum="$3"

        clickhouse_cmd "
            INSERT INTO ${CLICKHOUSE_DATABASE}.schema_migrations
            (version, name, checksum)
            VALUES (${version}, '${name}', '${checksum}')
        "
    }

    # Apply a single migration file
    apply_migration() {
        local migration_file="$1"
        local filename=$(basename "${migration_file}")

        # Extract version and name from filename (e.g., 001_initial_schema.sql)
        if [[ ! "${filename}" =~ ^([0-9]{3})_(.+)\.sql$ ]]; then
            log_warning "Skipping ${filename}: doesn't match naming convention {version}_{name}.sql"
            return 0
        fi

        local version="${BASH_REMATCH[1]}"
        local name="${BASH_REMATCH[2]}"
        local version_num=$((10#${version}))  # Convert to decimal, removing leading zeros
        local checksum=$(calculate_checksum "${migration_file}")

        # Check if already applied
        if is_migration_applied "${version_num}"; then
            log_info "Migration ${version}_${name} already applied, skipping"
            return 0
        fi

        log_info "Applying migration ${version}_${name}..."

        # Read and execute the migration file
        # We use --multiquery to allow multiple statements in one file
        local cmd="clickhouse-client ${CLICKHOUSE_CLIENT_EXTRA_ARGS} --host=${CLICKHOUSE_HOST} --port=${CLICKHOUSE_PORT} --user=${CLICKHOUSE_USER}"

        if [ -n "${CLICKHOUSE_PASSWORD}" ]; then
            cmd="${cmd} --password=${CLICKHOUSE_PASSWORD}"
        fi

        cmd="${cmd} --multiquery"

        if cat "${migration_file}" | ${cmd}; then
            # Record the migration as applied
            record_migration "${version_num}" "${name}" "${checksum}"
            log_success "Migration ${version}_${name} applied successfully"
            return 0
        else
            log_error "Failed to apply migration ${version}_${name}"
            return 1
        fi
    }

    # Apply all pending migrations
    apply_migrations() {
        log_info "Looking for migration files in ${MIGRATIONS_DIR}..."

        if [ ! -d "${MIGRATIONS_DIR}" ]; then
            log_error "Migrations directory ${MIGRATIONS_DIR} not found"
            return 1
        fi

        # Find all .sql files and sort them by version number
        # Note: ConfigMaps in Kubernetes mount files as symlinks, so we don't use -type f
        local migration_files=$(find "${MIGRATIONS_DIR}" -maxdepth 1 -name "*.sql" | sort)

        if [ -z "${migration_files}" ]; then
            log_warning "No migration files found in ${MIGRATIONS_DIR}"
            return 0
        fi

        local migrations_count=0
        local applied_count=0

        while IFS= read -r migration_file; do
            migrations_count=$((migrations_count + 1))
            if apply_migration "${migration_file}"; then
                applied_count=$((applied_count + 1))
            else
                log_error "Migration failed, stopping"
                return 1
            fi
        done <<< "${migration_files}"

        log_success "Migrations complete: ${applied_count} applied out of ${migrations_count} total"

        # Show current migration status
        show_migration_status
    }

    # Show current migration status
    show_migration_status() {
        log_info "Current migration status:"
        clickhouse_cmd "
            SELECT
                version,
                name,
                applied_at,
                substring(checksum, 1, 12) as checksum_short
            FROM ${CLICKHOUSE_DATABASE}.schema_migrations
            ORDER BY version
            FORMAT PrettyCompact
        " || log_warning "Could not fetch migration status"
    }

    # Verify schema matches expected state
    verify_schema() {
        log_info "Verifying schema..."

        # Check if audit.events table exists
        local events_table_exists=$(clickhouse_cmd "
            SELECT count()
            FROM system.tables
            WHERE database = '${CLICKHOUSE_DATABASE}' AND name = 'events'
        ")

        if [ "${events_table_exists}" -eq 0 ]; then
            log_error "Table ${CLICKHOUSE_DATABASE}.events does not exist!"
            return 1
        fi

        log_success "Schema verification passed"

        # Show table structure
        log_info "Table structure:"
        clickhouse_cmd "
            DESCRIBE TABLE ${CLICKHOUSE_DATABASE}.events
            FORMAT PrettyCompact
        " || true
    }

    # Main execution
    main() {
        log_info "ClickHouse Migration Runner Starting..."
        log_info "Target: ${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}"
        log_info "Database: ${CLICKHOUSE_DATABASE}"
        log_info "Migrations Directory: ${MIGRATIONS_DIR}"
        echo ""

        # Wait for ClickHouse to be ready
        if ! wait_for_clickhouse; then
            log_error "Failed to connect to ClickHouse"
            exit 1
        fi

        echo ""

        # Initialize migrations tracking
        if ! init_migrations_table; then
            log_error "Failed to initialize migrations table"
            exit 1
        fi

        echo ""

        # Apply all pending migrations
        if ! apply_migrations; then
            log_error "Migration process failed"
            exit 1
        fi

        echo ""

        # Verify schema
        if ! verify_schema; then
            log_error "Schema verification failed"
            exit 1
        fi

        echo ""
        log_success "All migrations completed successfully!"
    }

    # Run main function
    main "$@"

  001_initial_schema.sql: |
    -- Migration: 001_initial_schema
    -- Description: High-volume multi-tenant audit events table with projections for
    -- platform-wide querying and user-specific querying.
    -- Author: Scot Wells <swells@datum.net>
    -- Date: 2025-12-11

    CREATE DATABASE IF NOT EXISTS audit;

    CREATE TABLE IF NOT EXISTS audit.events
    (
        -- Raw audit event JSON
        event_json String CODEC(ZSTD(3)),

        -- Core timestamp (always queried)
        timestamp DateTime64(3) MATERIALIZED
            coalesce(
                parseDateTime64BestEffortOrNull(JSONExtractString(event_json, 'stageTimestamp')),
                now64(3)
            ),

        -- Scope annotations (multi-tenant scoping)
        scope_type LowCardinality(String) MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'annotations', 'platform.miloapis.com/scope.type'),
                ''
            ),

        scope_name String MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'annotations', 'platform.miloapis.com/scope.name'),
                ''
            ),

        -- User identity
        user String MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'user', 'username'),
                ''
            ),

        -- Request identity
        audit_id UUID MATERIALIZED
            toUUIDOrZero(coalesce(JSONExtractString(event_json, 'auditID'), '')),

        -- Common filters
        verb LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'verb'), ''),

        api_group LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'apiGroup'), ''),

        resource LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'resource'), ''),

        namespace LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'namespace'), ''),

        resource_name String MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'name'), ''),

        status_code UInt16 MATERIALIZED
            toUInt16OrZero(JSONExtractString(event_json, 'responseStatus', 'code')),

        -- ========================================================================
        -- Skip Indexes: Optimized for different query patterns
        -- ========================================================================

        -- Timestamp minmax index for time range queries
        INDEX idx_timestamp_minmax timestamp TYPE minmax GRANULARITY 4,

        -- Bloom filters with GRANULARITY 1 for high precision (critical filters)
        INDEX idx_verb_set            verb                  TYPE set(10) GRANULARITY 4,
        INDEX idx_resource_bloom      resource              TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX bf_api_resource         (api_group, resource) TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_verb_resource_bloom (verb, resource)      TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_user_bloom          user                  TYPE bloom_filter(0.001) GRANULARITY 1,

        -- Set indexes for low-cardinality columns
        INDEX idx_status_code_set status_code TYPE set(100) GRANULARITY 4,
        -- Minmax index for status_code range queries
        INDEX idx_status_code_minmax status_code TYPE minmax GRANULARITY 4,
    )
    ENGINE = ReplacingMergeTree
    PARTITION BY toYYYYMMDD(timestamp)
    -- Primary key optimized for tenant-scoped queries
    -- Deduplication occurs on the full ORDER BY key during merges
    ORDER BY (timestamp, scope_type, scope_name, user, audit_id)

    -- Move parts to cold S3-backed volume after 90 days
    TTL timestamp + INTERVAL 90 DAY TO VOLUME 'cold'

    SETTINGS
        storage_policy = 'hot_cold',
        ttl_only_drop_parts = 1,
        deduplicate_merge_projection_mode = 'rebuild';

    -- ============================================================================
    -- Step 3: Add Platform Query Projection
    -- ============================================================================
    -- This projection is optimized for platform-wide queries that filter by
    -- timestamp, api_group, and resource (common for cross-tenant analytics).
    --
    -- Sort order: (timestamp, api_group, resource, audit_id)
    -- Use cases:
    --   - "All events for 'apps' API group and 'deployments' resource in last 24 hours"
    --   - "All events for core API 'pods' resource"
    --   - Platform-wide verb/resource filtering
    --

    ALTER TABLE audit.events
    ADD PROJECTION platform_query_projection
    (
        SELECT *
        ORDER BY (timestamp, api_group, resource, audit_id)
    );

    -- ============================================================================
    -- Step 4: Add User Query Projection
    -- ============================================================================
    -- This projection is optimized for user-specific queries within time ranges.
    --
    -- Sort order: (timestamp, user, api_group, resource)
    -- Use cases:
    --   - "What did alice@example.com do in the last 24 hours?"
    --   - "All events by system:serviceaccount:kube-system:default"
    --   - User-specific verb/resource filtering
    --
    -- ClickHouse automatically chooses the best projection for each query based
    -- on the WHERE clause filters.

    ALTER TABLE audit.events
    ADD PROJECTION user_query_projection
    (
        SELECT *
        ORDER BY (timestamp, user, api_group, resource)
    );

