# AUTO-GENERATED - DO NOT EDIT MANUALLY
# Generated from migrations/ directory (source of truth)
# To regenerate: task migrations:generate OR ./hack/generate-migrations-configmap.sh
#
# To add a new migration:
# 1. Create file in migrations/ (e.g., migrations/002_add_field.sql)
# 2. Run: task migrations:generate
# 3. Update job.yaml to include the new migration in volumes
# 4. Deploy: task dev:deploy

apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-migrations
  namespace: activity-system
  labels:
    app: clickhouse-migrations
    app.kubernetes.io/component: database
data:
  # Migration runner script
  migrate.sh: |
    #!/bin/bash
    set -euo pipefail

    # ClickHouse Migration Runner
    # This script applies versioned SQL migrations to a ClickHouse database
    # It tracks applied migrations in the audit.schema_migrations table

    # Configuration from environment variables
    CLICKHOUSE_HOST="${CLICKHOUSE_HOST:-clickhouse}"
    CLICKHOUSE_PORT="${CLICKHOUSE_PORT:-9000}"
    CLICKHOUSE_USER="${CLICKHOUSE_USER:-default}"
    CLICKHOUSE_PASSWORD="${CLICKHOUSE_PASSWORD:-}"
    CLICKHOUSE_DATABASE="${CLICKHOUSE_DATABASE:-audit}"
    MIGRATIONS_DIR="${MIGRATIONS_DIR:-/migrations}"
    CLICKHOUSE_SECURE="${CLICKHOUSE_SECURE:-false}"
    CLICKHOUSE_CLIENT_EXTRA_ARGS="${CLICKHOUSE_CLIENT_EXTRA_ARGS:-}"

    # Colors for output
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    BLUE='\033[0;34m'
    NC='\033[0m' # No Color

    log_info() {
        echo -e "${BLUE}[INFO]${NC} $1"
    }

    log_success() {
        echo -e "${GREEN}[SUCCESS]${NC} $1"
    }

    log_warning() {
        echo -e "${YELLOW}[WARNING]${NC} $1"
    }

    log_error() {
        echo -e "${RED}[ERROR]${NC} $1"
    }

    # Build clickhouse-client command with authentication
    clickhouse_cmd() {
        local query="$1"
        local cmd="clickhouse-client ${CLICKHOUSE_CLIENT_EXTRA_ARGS} --host=${CLICKHOUSE_HOST} --port=${CLICKHOUSE_PORT} --user=${CLICKHOUSE_USER}"

        if [ -n "${CLICKHOUSE_PASSWORD}" ]; then
            cmd="${cmd} --password=${CLICKHOUSE_PASSWORD}"
        fi

        echo "${query}" | ${cmd}
    }

    # Wait for ClickHouse to be ready
    wait_for_clickhouse() {
        log_info "Waiting for ClickHouse to be ready at ${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}..."

        local max_attempts=30
        local attempt=1

        while [ $attempt -le $max_attempts ]; do
            if clickhouse_cmd "SELECT 1" &>/dev/null; then
                log_success "ClickHouse is ready!"
                return 0
            fi

            log_info "Attempt $attempt/$max_attempts: ClickHouse not ready yet, waiting..."
            sleep 2
            attempt=$((attempt + 1))
        done

        log_error "ClickHouse did not become ready within the timeout period"
        return 1
    }

    # Wait for all replicas in the cluster to be healthy and ready
    # This function will wait indefinitely until all replicas are online and healthy
    wait_for_cluster_ready() {
        log_info "Waiting for all 3 replicas in the 'activity' cluster to be ready..."
        log_info "This will wait indefinitely until the cluster is healthy."

        local expected_replicas=3
        local attempt=1

        while true; do
            # Check if the 'activity' cluster exists and has the expected number of replicas
            local cluster_exists=$(clickhouse_cmd "SELECT count() FROM system.clusters WHERE cluster='activity'" 2>/dev/null || echo "0")

            if [ "$cluster_exists" -eq 0 ]; then
                log_info "Attempt $attempt: 'activity' cluster not yet registered in system.clusters, waiting..."
                sleep 5
                attempt=$((attempt + 1))
                continue
            fi

            # Get total number of replicas in the cluster
            local total_replicas=$(clickhouse_cmd "SELECT count() FROM system.clusters WHERE cluster='activity'" 2>/dev/null || echo "0")

            # Get number of healthy replicas (errors_count=0 means no connection errors)
            local healthy_replicas=$(clickhouse_cmd "SELECT count() FROM system.clusters WHERE cluster='activity' AND errors_count=0" 2>/dev/null || echo "0")

            # Check if we have the expected number of replicas and all are healthy
            if [ "$total_replicas" -eq "$expected_replicas" ] && [ "$healthy_replicas" -eq "$expected_replicas" ]; then
                log_success "All $expected_replicas replicas are registered and healthy!"

                # Additional check: verify Keeper connectivity for distributed DDL
                log_info "Verifying ClickHouse Keeper connectivity for distributed DDL..."
                if clickhouse_cmd "SELECT count() FROM system.zookeeper WHERE path='/clickhouse/activity'" &>/dev/null; then
                    log_success "ClickHouse Keeper is accessible and cluster coordination is ready!"

                    # Final verification: display cluster topology
                    log_info "Cluster topology:"
                    clickhouse_cmd "
                        SELECT
                            cluster,
                            shard_num,
                            replica_num,
                            host_name,
                            port,
                            errors_count
                        FROM system.clusters
                        WHERE cluster = 'activity'
                        ORDER BY shard_num, replica_num
                        FORMAT PrettyCompact
                    " || true

                    return 0
                else
                    log_info "Attempt $attempt: Keeper connectivity not ready yet, waiting..."
                fi
            else
                log_info "Attempt $attempt: $healthy_replicas/$total_replicas healthy replicas (expected: $expected_replicas), waiting..."
            fi

            sleep 5
            attempt=$((attempt + 1))
        done
    }

    # Initialize the schema_migrations table if it doesn't exist
    init_migrations_table() {
        log_info "Verifying schema_migrations table..."

        # Note: Both database and schema_migrations table creation are handled by the
        # first migration (001_initial_schema.sql). This function simply verifies
        # the table exists before we try to query it for already-applied migrations.
        #
        # We don't create it here because:
        # 1. The table should be created with the Replicated database engine for HA
        # 2. All schema changes should go through the migration system for consistency
        # 3. The first migration will create both the database and this tracking table

        # Check if the table exists (will be created by first migration if not)
        local table_exists=$(clickhouse_cmd "
            SELECT count()
            FROM system.tables
            WHERE database = '${CLICKHOUSE_DATABASE}' AND name = 'schema_migrations'
        " 2>/dev/null || echo "0")

        if [ "${table_exists}" -eq 0 ]; then
            log_info "Schema migrations table does not exist yet - will be created by first migration"
        else
            log_success "Schema migrations table exists and is ready"
        fi
    }

    # Calculate checksum of a file
    calculate_checksum() {
        local file="$1"
        sha256sum "${file}" | awk '{print $1}'
    }

    # Check if a migration has already been applied
    is_migration_applied() {
        local version="$1"

        # If the schema_migrations table doesn't exist yet, no migrations have been applied
        # This handles the case for the very first migration which creates the table
        local result=$(clickhouse_cmd "
            SELECT count(*)
            FROM ${CLICKHOUSE_DATABASE}.schema_migrations
            WHERE version = ${version}
        " 2>/dev/null || echo "0")

        [ "${result}" -gt 0 ]
    }

    # Record a migration as applied
    record_migration() {
        local version="$1"
        local name="$2"
        local checksum="$3"

        clickhouse_cmd "
            INSERT INTO ${CLICKHOUSE_DATABASE}.schema_migrations
            (version, name, checksum)
            VALUES (${version}, '${name}', '${checksum}')
        "
    }

    # Apply a single migration file
    apply_migration() {
        local migration_file="$1"
        local filename=$(basename "${migration_file}")

        # Extract version and name from filename (e.g., 001_initial_schema.sql)
        if [[ ! "${filename}" =~ ^([0-9]{3})_(.+)\.sql$ ]]; then
            log_warning "Skipping ${filename}: doesn't match naming convention {version}_{name}.sql"
            return 0
        fi

        local version="${BASH_REMATCH[1]}"
        local name="${BASH_REMATCH[2]}"
        local version_num=$((10#${version}))  # Convert to decimal, removing leading zeros
        local checksum=$(calculate_checksum "${migration_file}")

        # Check if already applied
        if is_migration_applied "${version_num}"; then
            log_info "Migration ${version}_${name} already applied, skipping"
            return 0
        fi

        log_info "Applying migration ${version}_${name}..."

        # Read and execute the migration file
        # We use --multiquery to allow multiple statements in one file
        local cmd="clickhouse-client ${CLICKHOUSE_CLIENT_EXTRA_ARGS} --host=${CLICKHOUSE_HOST} --port=${CLICKHOUSE_PORT} --user=${CLICKHOUSE_USER}"

        if [ -n "${CLICKHOUSE_PASSWORD}" ]; then
            cmd="${cmd} --password=${CLICKHOUSE_PASSWORD}"
        fi

        cmd="${cmd} --multiquery"

        if cat "${migration_file}" | ${cmd}; then
            # Record the migration as applied
            record_migration "${version_num}" "${name}" "${checksum}"
            log_success "Migration ${version}_${name} applied successfully"
            return 0
        else
            log_error "Failed to apply migration ${version}_${name}"
            return 1
        fi
    }

    # Apply all pending migrations
    apply_migrations() {
        log_info "Looking for migration files in ${MIGRATIONS_DIR}..."

        if [ ! -d "${MIGRATIONS_DIR}" ]; then
            log_error "Migrations directory ${MIGRATIONS_DIR} not found"
            return 1
        fi

        # Find all .sql files and sort them by version number
        # Note: ConfigMaps in Kubernetes mount files as symlinks, so we don't use -type f
        local migration_files=$(find "${MIGRATIONS_DIR}" -maxdepth 1 -name "*.sql" | sort)

        if [ -z "${migration_files}" ]; then
            log_warning "No migration files found in ${MIGRATIONS_DIR}"
            return 0
        fi

        local migrations_count=0
        local applied_count=0

        while IFS= read -r migration_file; do
            migrations_count=$((migrations_count + 1))
            if apply_migration "${migration_file}"; then
                applied_count=$((applied_count + 1))
            else
                log_error "Migration failed, stopping"
                return 1
            fi
        done <<< "${migration_files}"

        log_success "Migrations complete: ${applied_count} applied out of ${migrations_count} total"

        # Show current migration status
        show_migration_status
    }

    # Show current migration status
    show_migration_status() {
        log_info "Current migration status:"
        clickhouse_cmd "
            SELECT
                version,
                name,
                applied_at,
                substring(checksum, 1, 12) as checksum_short
            FROM ${CLICKHOUSE_DATABASE}.schema_migrations
            ORDER BY version
            FORMAT PrettyCompact
        " || log_warning "Could not fetch migration status"
    }

    # Verify schema matches expected state
    verify_schema() {
        log_info "Verifying schema..."

        # Check if audit.events table exists
        local events_table_exists=$(clickhouse_cmd "
            SELECT count()
            FROM system.tables
            WHERE database = '${CLICKHOUSE_DATABASE}' AND name = 'events'
        ")

        if [ "${events_table_exists}" -eq 0 ]; then
            log_error "Table ${CLICKHOUSE_DATABASE}.events does not exist!"
            return 1
        fi

        log_success "Schema verification passed"

        # Show table structure
        log_info "Table structure:"
        clickhouse_cmd "
            DESCRIBE TABLE ${CLICKHOUSE_DATABASE}.events
            FORMAT PrettyCompact
        " || true
    }

    # Main execution
    main() {
        log_info "ClickHouse Migration Runner Starting..."
        log_info "Target: ${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}"
        log_info "Database: ${CLICKHOUSE_DATABASE}"
        log_info "Migrations Directory: ${MIGRATIONS_DIR}"
        echo ""

        log_info "IMPORTANT: This migration script should only run against a single replica."
        log_info "The Replicated database engine automatically propagates DDL changes to all replicas."
        echo ""

        # Wait for ClickHouse to be ready
        if ! wait_for_clickhouse; then
            log_error "Failed to connect to ClickHouse"
            exit 1
        fi

        # Display which replica we're connected to
        log_info "Connected to replica:"
        clickhouse_cmd "SELECT hostName() as host, getMacro('replica') as replica_name" || true

        echo ""

        # Wait for all cluster replicas to be healthy
        if ! wait_for_cluster_ready; then
            log_error "Cluster is not fully healthy"
            exit 1
        fi

        echo ""

        # Verify migrations tracking (table will be created by first migration)
        init_migrations_table

        echo ""

        # Apply all pending migrations
        if ! apply_migrations; then
            log_error "Migration process failed"
            exit 1
        fi

        echo ""

        # Verify schema
        if ! verify_schema; then
            log_error "Schema verification failed"
            exit 1
        fi

        echo ""
        log_success "All migrations completed successfully!"
    }

    # Run main function
    main "$@"

  001_initial_schema.sql: |
    -- Migration: 001_initial_schema
    -- Description: High-volume multi-tenant audit events table with HA replication
    -- and projections for platform-wide querying and user-specific querying.
    -- Author: Scot Wells <swells@datum.net>
    -- Date: 2025-12-11
    -- Updated: 2026-01-15 - Added HA replication support with ReplicatedReplacingMergeTree
    -- Updated: 2026-01-16 - Use Replicated database engine for automatic DDL replication

    -- ============================================================================
    -- Step 1: Create Replicated Database
    -- ============================================================================
    -- The Replicated database engine automatically replicates all DDL operations
    -- across all replicas in the cluster. This ensures schema consistency without
    -- requiring ON CLUSTER clauses.
    --
    -- UUID ensures the database has the same identifier on all replicas
    -- Path: /clickhouse/activity/databases/audit in ClickHouse Keeper
    -- Macros: {shard} and {replica} are automatically substituted by ClickHouse
    CREATE DATABASE IF NOT EXISTS audit ON CLUSTER 'activity'
    ENGINE = Replicated('/clickhouse/activity/databases/audit', '{shard}', '{replica}');

    -- ============================================================================
    -- Step 2: Create Schema Migrations Tracking Table
    -- ============================================================================
    -- This table tracks which migrations have been applied to prevent re-running
    -- them. Each migration records its version, name, application timestamp, and
    -- checksum for integrity verification.
    CREATE TABLE IF NOT EXISTS audit.schema_migrations
    (
        version UInt32,
        name String,
        applied_at DateTime64(3) DEFAULT now64(3),
        checksum String
    ) ENGINE = ReplicatedReplacingMergeTree()
    ORDER BY version
    SETTINGS
        -- No special storage policy needed for this small metadata table
        storage_policy = 'default';

    -- ============================================================================
    -- Step 3: Create Events Table
    -- ============================================================================
    -- Replicated database automatically replicates table DDL - no need for ON CLUSTER
    CREATE TABLE IF NOT EXISTS audit.events
    (
        -- Raw audit event JSON
        event_json String CODEC(ZSTD(3)),

        -- Core timestamp (always queried)
        -- Uses requestReceivedTimestamp which represents when the API server received the request.
        timestamp DateTime64(3) MATERIALIZED
            coalesce(
                parseDateTime64BestEffortOrNull(JSONExtractString(event_json, 'requestReceivedTimestamp')),
                now64(3)
            ),

        -- Scope annotations (multi-tenant scoping)
        scope_type LowCardinality(String) MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'annotations', 'platform.miloapis.com/scope.type'),
                ''
            ),

        scope_name String MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'annotations', 'platform.miloapis.com/scope.name'),
                ''
            ),

        -- User identity
        user String MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'user', 'username'),
                ''
            ),

        user_uid String MATERIALIZED
            coalesce(
                JSONExtractString(event_json, 'user', 'uid'),
                ''
            ),

        -- Request identity
        audit_id UUID MATERIALIZED
            toUUIDOrZero(coalesce(JSONExtractString(event_json, 'auditID'), '')),

        -- Common filters
        verb LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'verb'), ''),

        api_group LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'apiGroup'), ''),

        resource LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'resource'), ''),

        namespace LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'namespace'), ''),

        resource_name String MATERIALIZED
            coalesce(JSONExtractString(event_json, 'objectRef', 'name'), ''),

        status_code UInt16 MATERIALIZED
            toUInt16OrZero(JSONExtractString(event_json, 'responseStatus', 'code')),

        -- ========================================================================
        -- Skip Indexes: Optimized for different query patterns
        -- ========================================================================

        -- Timestamp minmax index for time range queries
        INDEX idx_timestamp_minmax timestamp TYPE minmax GRANULARITY 4,

        -- Bloom filters with GRANULARITY 1 for high precision (critical filters)
        INDEX idx_verb_set            verb                  TYPE set(10) GRANULARITY 4,
        INDEX idx_resource_bloom      resource              TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX bf_api_resource         (api_group, resource) TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_verb_resource_bloom (verb, resource)      TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_user_bloom          user                  TYPE bloom_filter(0.001) GRANULARITY 1,
        INDEX idx_user_uid_bloom      user_uid              TYPE bloom_filter(0.001) GRANULARITY 1,

        -- Set indexes for low-cardinality columns
        INDEX idx_status_code_set status_code TYPE set(100) GRANULARITY 4,
        -- Minmax index for status_code range queries
        INDEX idx_status_code_minmax status_code TYPE minmax GRANULARITY 4,
    )
    -- ==================================================================
    -- TABLE ENGINE CONFIGURATION (High Availability)
    -- ==================================================================
    -- ReplicatedReplacingMergeTree provides:
    -- - Deduplication based on ORDER BY key during merges
    -- - Eventual consistency with quorum writes (configured via settings)
    -- - Data replication to other database replicas
    --
    -- Replication Behavior:
    -- - INSERT on any replica replicates to all replicas via Keeper (database-level)
    -- - Quorum writes ensure 2/3 replicas acknowledge before success
    -- - Deduplication happens during background merges
    ENGINE = ReplicatedReplacingMergeTree
    PARTITION BY toYYYYMMDD(timestamp)
    -- Primary key optimized for tenant-scoped queries with hour bucketing
    -- Hour bucketing provides data locality while timestamp ensures strict chronological order
    -- Timestamp as second key ensures events are always returned in time order for audit compliance
    -- Deduplication occurs on the full ORDER BY key during merges
    ORDER BY (toStartOfHour(timestamp), timestamp, scope_type, scope_name, user, audit_id)
    PRIMARY KEY (toStartOfHour(timestamp), timestamp, scope_type, scope_name, user, audit_id)

    -- Move parts to cold S3-backed volume after 90 days
    TTL timestamp + INTERVAL 90 DAY TO VOLUME 'cold'

    SETTINGS
        storage_policy = 'hot_cold',
        ttl_only_drop_parts = 1,
        deduplicate_merge_projection_mode = 'rebuild';

    -- ============================================================================
    -- Step 4: Add Platform Query Projection
    -- ============================================================================
    -- This projection is optimized for platform-wide queries that filter by
    -- timestamp, api_group, and resource (common for cross-tenant analytics).
    --
    -- Sort order: (toStartOfHour(timestamp), timestamp, api_group, resource, audit_id)
    -- Use cases:
    --   - "All events for 'apps' API group and 'deployments' resource in last 24 hours"
    --   - "All events for core API 'pods' resource"
    --   - Platform-wide verb/resource filtering
    --
    -- Hour bucketing provides index efficiency while timestamp ensures strict chronological order.
    -- Timestamp as second key ensures events are always returned in time order for audit compliance.

    ALTER TABLE audit.events
    ADD PROJECTION platform_query_projection
    (
        SELECT *
        ORDER BY (toStartOfHour(timestamp), timestamp, api_group, resource, audit_id)
    );

    -- ============================================================================
    -- Step 5: Add User Query Projection
    -- ============================================================================
    -- This projection is optimized for username-based queries within time ranges.
    --
    -- Sort order: (toStartOfHour(timestamp), timestamp, user, api_group, resource, audit_id)
    -- Use cases:
    --   - "What did alice@example.com do in the last 24 hours?"
    --   - "All events by system:serviceaccount:kube-system:default"
    --   - Platform admin filtering by username in CEL expressions
    --
    -- Hour bucketing provides index efficiency while timestamp ensures strict chronological order.
    -- Timestamp as second key ensures events are always returned in time order for audit compliance.
    -- ClickHouse automatically chooses the best projection for each query based
    -- on the WHERE clause filters.

    ALTER TABLE audit.events
    ADD PROJECTION user_query_projection
    (
        SELECT *
        ORDER BY (toStartOfHour(timestamp), timestamp, user, api_group, resource, audit_id)
    );

    -- ============================================================================
    -- Step 6: Add User UID Query Projection
    -- ============================================================================
    -- This projection is optimized for user-scoped queries by UID.
    --
    -- Sort order: (toStartOfHour(timestamp), timestamp, user_uid, api_group, resource, audit_id)
    -- Use cases:
    --   - User-scoped queries: "Show all activity by user with UID abc-123"
    --   - Cross-organization user activity tracking
    --   - User-specific audit trail regardless of username changes
    --
    -- This projection is used when scope.type == "user" to filter by user_uid
    -- instead of scope_name, enabling queries for a user's activity across all
    -- organizations and projects on the platform.
    --
    -- Hour bucketing provides index efficiency while timestamp ensures strict chronological order.
    -- Timestamp as second key ensures events are always returned in time order for audit compliance.

    ALTER TABLE audit.events
    ADD PROJECTION user_uid_query_projection
    (
        SELECT *
        ORDER BY (toStartOfHour(timestamp), timestamp, user_uid, api_group, resource, audit_id)
    );

  002_activities_table.sql: |
    -- Migration: 002_activities_table
    -- Description: Activities table for storing translated activity records
    -- Author: Activity System
    -- Date: 2026-02-02

    -- ============================================================================
    -- Activities Table
    -- ============================================================================
    -- Stores translated activity records generated by the Activity Processor.
    -- These are human-readable summaries of audit logs and Kubernetes events,
    -- optimized for time-range queries and multi-tenant isolation.

    CREATE TABLE IF NOT EXISTS audit.activities
    (
        -- Full activity record as JSON (compressed)
        activity_json String CODEC(ZSTD(3)),

        -- Core timestamp for time-range queries
        timestamp DateTime64(3) MATERIALIZED
            coalesce(
                parseDateTime64BestEffortOrNull(JSONExtractString(activity_json, 'metadata', 'creationTimestamp')),
                now64(3)
            ),

        -- Multi-tenant isolation
        tenant_type LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'tenant', 'type'), ''),

        tenant_name String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'tenant', 'name'), ''),

        -- Origin tracking for correlation to source records
        origin_type LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'origin', 'type'), ''),

        origin_id String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'origin', 'id'), ''),

        -- Change source classification (human vs system)
        change_source LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'changeSource'), ''),

        -- Actor information
        actor_type LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'actor', 'type'), ''),

        actor_name String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'actor', 'name'), ''),

        actor_uid String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'actor', 'uid'), ''),

        -- Resource information
        api_group LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'resource', 'apiGroup'), ''),

        resource_kind LowCardinality(String) MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'resource', 'kind'), ''),

        resource_name String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'resource', 'name'), ''),

        resource_namespace String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'resource', 'namespace'), ''),

        resource_uid String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'resource', 'uid'), ''),

        -- Activity metadata
        activity_name String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'metadata', 'name'), ''),

        activity_namespace String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'metadata', 'namespace'), ''),

        -- Summary for full-text search
        summary String MATERIALIZED
            coalesce(JSONExtractString(activity_json, 'spec', 'summary'), ''),

        -- ========================================================================
        -- Skip Indexes
        -- ========================================================================

        -- Bloom filter for API group filtering (service provider queries)
        INDEX idx_api_group api_group TYPE bloom_filter(0.01) GRANULARITY 1,

        -- Bloom filter for actor-based filtering
        INDEX idx_actor_name actor_name TYPE bloom_filter(0.001) GRANULARITY 1,
        INDEX idx_actor_uid actor_uid TYPE bloom_filter(0.001) GRANULARITY 1,

        -- Bloom filter for resource lookups
        INDEX idx_resource resource_kind TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_resource_name resource_name TYPE bloom_filter(0.01) GRANULARITY 1,
        INDEX idx_resource_uid resource_uid TYPE bloom_filter(0.001) GRANULARITY 1,

        -- Minmax for change source filtering (human vs system)
        INDEX idx_change_source change_source TYPE set(10) GRANULARITY 4,

        -- Full-text index for summary search (ngrams enable substring/prefix matching)
        INDEX idx_summary_search summary TYPE text(tokenizer = ngrams(3)) GRANULARITY 1,

        -- ========================================================================
        -- Projections (defined inline for ReplicatedReplacingMergeTree compatibility)
        -- ========================================================================

        -- Projection for service provider queries (by API group)
        PROJECTION api_group_query_projection
        (
            SELECT *
            ORDER BY (api_group, timestamp, tenant_type, tenant_name, resource_uid)
        ),

        -- Projection for actor-based queries
        PROJECTION actor_query_projection
        (
            SELECT *
            ORDER BY (actor_name, timestamp, tenant_type, tenant_name, resource_uid)
        )
    )
    ENGINE = ReplicatedReplacingMergeTree
    PARTITION BY toYYYYMMDD(timestamp)
    -- Primary key optimized for tenant-scoped time-range queries
    ORDER BY (tenant_type, tenant_name, timestamp, resource_uid)
    PRIMARY KEY (tenant_type, tenant_name, timestamp, resource_uid)

    -- 60-day retention for activities
    TTL timestamp + INTERVAL 60 DAY DELETE

    SETTINGS
        storage_policy = 'default',
        ttl_only_drop_parts = 1,
        deduplicate_merge_projection_mode = 'rebuild';

